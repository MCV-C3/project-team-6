wandb: WARNING The anonymous setting has no effect and will be removed in a future version.
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /ghome/group06/.netrc.
wandb: Currently logged in as: arnaumarcosalmansa (mcv-team-6) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run gfkdn1la
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in wandb/run-20260118_165606-gfkdn1la
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Test run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/mcv-team-6/C3-Week4
wandb: üöÄ View run at https://wandb.ai/mcv-team-6/C3-Week4/runs/gfkdn1la
/ghome/group06/miniconda3/envs/c3/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python train.py ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type               | Params | Mode  | FLOPs
------------------------------------------------------------------
0 | model      | SmallLeNet         | 24.3 K | train | 0    
1 | loss_fn    | CrossEntropyLoss   | 0      | train | 0    
2 | train_acc  | MulticlassAccuracy | 0      | train | 0    
3 | val_acc    | MulticlassAccuracy | 0      | train | 0    
4 | val_loss   | MeanMetric         | 0      | train | 0    
5 | train_loss | MeanMetric         | 0      | train | 0    
------------------------------------------------------------------
24.3 K    Trainable params
0         Non-trainable params
24.3 K    Total params
0.097     Total estimated model params size (MB)
19        Modules in train mode
0         Modules in eval mode
0         Total Flops
/ghome/group06/miniconda3/envs/c3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:317: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
