wandb: WARNING The anonymous setting has no effect and will be removed in a future version.
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /ghome/group06/.netrc.
wandb: Currently logged in as: arnaumarcosalmansa (mcv-team-6) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run nr2nl1a3
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in wandb/run-20260118_165428-nr2nl1a3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Test run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/mcv-team-6/C3-Week4
wandb: üöÄ View run at https://wandb.ai/mcv-team-6/C3-Week4/runs/nr2nl1a3
/ghome/group06/miniconda3/envs/c3/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python train.py ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type               | Params | Mode  | FLOPs
------------------------------------------------------------------
0 | model      | SmallLeNet         | 24.3 K | train | 0    
1 | loss_fn    | CrossEntropyLoss   | 0      | train | 0    
2 | train_acc  | MulticlassAccuracy | 0      | train | 0    
3 | val_acc    | MulticlassAccuracy | 0      | train | 0    
4 | val_loss   | MeanMetric         | 0      | train | 0    
5 | train_loss | MeanMetric         | 0      | train | 0    
------------------------------------------------------------------
24.3 K    Trainable params
0         Non-trainable params
24.3 K    Total params
0.097     Total estimated model params size (MB)
19        Modules in train mode
0         Modules in eval mode
0         Total Flops
/ghome/group06/miniconda3/envs/c3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:317: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Traceback (most recent call last):
  File "/export/home/group06/workspace/project-team-6/Week4/train.py", line 47, in <module>
    trainer.fit(train_model, train_loader, test_loader)
  File "/ghome/group06/miniconda3/envs/c3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 584, in fit
    call._call_and_handle_interrupt(
  File "/ghome/group06/miniconda3/envs/c3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ghome/group06/miniconda3/envs/c3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 630, in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
  File "/ghome/group06/miniconda3/envs/c3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 1079, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/ghome/group06/miniconda3/envs/c3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 1123, in _run_stage
    self.fit_loop.run()
  File "/ghome/group06/miniconda3/envs/c3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py", line 218, in run
    self.on_advance_end()
  File "/ghome/group06/miniconda3/envs/c3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py", line 480, in on_advance_end
    call._call_callback_hooks(trainer, "on_train_epoch_end", monitoring_callbacks=True)
  File "/ghome/group06/miniconda3/envs/c3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 228, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/ghome/group06/miniconda3/envs/c3/lib/python3.11/site-packages/pytorch_lightning/callbacks/early_stopping.py", line 217, in on_train_epoch_end
    self._run_early_stopping_check(trainer)
  File "/ghome/group06/miniconda3/envs/c3/lib/python3.11/site-packages/pytorch_lightning/callbacks/early_stopping.py", line 229, in _run_early_stopping_check
    if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ghome/group06/miniconda3/envs/c3/lib/python3.11/site-packages/pytorch_lightning/callbacks/early_stopping.py", line 175, in _validate_condition_metric
    raise RuntimeError(error_msg)
RuntimeError: Early stopping conditioned on metric `test_loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `val_loss`, `val_acc`, `train_loss`, `train_acc`
